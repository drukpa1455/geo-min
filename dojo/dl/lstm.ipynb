{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) architecture that is widely used for modeling sequential data, including time-series data. LSTM networks are designed to capture long-term dependencies and mitigate the vanishing gradient problem that occurs in traditional RNNs.\n",
    "\n",
    "LSTM representations refer to the learned representations or hidden states of an LSTM network when processing time-series data. These representations capture meaningful and informative features from the input sequence and can be used for various downstream tasks, such as sequence prediction, classification, or anomaly detection.\n",
    "\n",
    "Here are key aspects of LSTM representations:\n",
    "\n",
    "1. Memory Cells: LSTMs incorporate memory cells that allow the network to remember and selectively retain information over longer sequences. These memory cells are responsible for capturing and storing relevant information from past time steps.\n",
    "\n",
    "2. Forget Gate: LSTMs employ a forget gate mechanism that determines how much of the previous memory to forget and update based on the current input. The forget gate learns to discard irrelevant or outdated information from the memory cells.\n",
    "\n",
    "3. Input and Output Gates: LSTMs have input and output gates that regulate the flow of information into and out of the memory cells. The input gate determines how much of the new input to incorporate into the memory cells, while the output gate controls the flow of information from the memory cells to the output.\n",
    "\n",
    "4. Hidden States: LSTMs produce hidden states at each time step, which represent the learned representations of the input sequence. These hidden states capture the relevant information and dependencies in the sequence. The hidden state at the final time step can be considered as the LSTM representation of the entire input sequence.\n",
    "\n",
    "5. Depth and Stacked LSTMs: LSTMs can be stacked to create deeper networks, where the hidden states of one LSTM layer serve as inputs to the next layer. Stacked LSTMs allow for the learning of more complex and hierarchical representations, potentially capturing more nuanced patterns in the data.\n",
    "\n",
    "6. Transfer Learning: LSTM representations learned from one task or dataset can be transferred to another related task or dataset. By leveraging the learned LSTM representations, transfer learning can help improve performance and accelerate training on new tasks with limited data.\n",
    "\n",
    "LSTM representations are powerful for modeling time-series data because they can capture long-term dependencies, handle variable-length sequences, and automatically learn informative features from the data. These representations have been successfully applied in various domains, including natural language processing, speech recognition, and financial forecasting, among others.\n",
    "\n",
    "By leveraging LSTM representations, practitioners can benefit from the ability of LSTMs to model temporal dependencies and extract relevant features, making them valuable for tasks such as sequence prediction, time-series classification, sentiment analysis, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass through the LSTM layers\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply fully connected layer to the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 10, 1])\n",
      "Representations shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "sequence_length = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Generate random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Compute LSTM representations\n",
    "representations = lstm_model(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Representations shape:\", representations.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Head Attention\n",
    "Let's denote the input sequence as X = [x₁, x₂, ..., xₙ], where each element xᵢ represents a token or feature. We can break down the attention mechanism into mathematical steps as follows:\n",
    "\n",
    "1. Query, Key, and Value:\n",
    "   - Query: q = Wq · X, where Wq is a learnable weight matrix.\n",
    "   - Key: K = WK · X, where WK is a learnable weight matrix.\n",
    "   - Value: V = WV · X, where WV is a learnable weight matrix.\n",
    "\n",
    "   Here, q, K, and V are the query, key, and value vectors, respectively.\n",
    "\n",
    "2. Similarity Calculation:\n",
    "   - Compute the similarity scores between the query and key vectors, denoted as S = qᵀ · K.\n",
    "   \n",
    "   The similarity scores S measure the relevance between the query and each key-value pair.\n",
    "\n",
    "3. Attention Weights:\n",
    "   - Apply softmax to the similarity scores to obtain attention weights, denoted as A = softmax(S).\n",
    "   \n",
    "   Softmax ensures that the attention weights sum up to 1, representing the importance or contribution of each key-value pair.\n",
    "\n",
    "4. Weighted Sum:\n",
    "   - Compute the weighted sum of the value vectors using the attention weights, denoted as C = A · V.\n",
    "   \n",
    "   The weighted sum C represents the context or attended information, where more weight is given to the value vectors that have higher attention weights.\n",
    "\n",
    "In summary, the attention mechanism computes attention weights by comparing the query vector with the key vectors. These weights are then used to compute a weighted sum of the value vectors, which represents the attended information or context. This mechanism allows the model to selectively focus on different parts of the input sequence based on their relevance to the query.\n",
    "\n",
    "Note that the above equations provide a general outline of the attention mechanism. Different variants and extensions of attention may incorporate additional components or modifications based on the specific task or architecture being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        \n",
    "        self.query_transform = nn.Linear(input_size, input_size)\n",
    "        self.key_transform = nn.Linear(input_size, input_size)\n",
    "        self.value_transform = nn.Linear(input_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute query, key, and value vectors\n",
    "        query = self.query_transform(x)\n",
    "        key = self.key_transform(x)\n",
    "        value = self.value_transform(x)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # Compute attention weights using softmax\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        weighted_sum = torch.matmul(weights, value)\n",
    "        \n",
    "        return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 10, 64])\n",
      "Output shape: torch.Size([32, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 64\n",
    "sequence_length = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Generate random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "# Create and apply single-head attention\n",
    "attention = SingleHeadAttention(input_size)\n",
    "output = attention(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head\n",
    "Multi-head attention extends the basic attention mechanism by incorporating multiple attention heads. Each attention head performs its own attention computation independently. Here's the mathematical formulation for multi-head attention:\n",
    "\n",
    "Given an input sequence X = [x₁, x₂, ..., xₙ], the multi-head attention mechanism can be broken down into the following steps:\n",
    "\n",
    "1. Input Transformation:\n",
    "   - Perform linear transformations to obtain query (Q), key (K), and value (V) vectors for each attention head.\n",
    "   - Let Qᵢ = WQi · X, Kᵢ = WKi · X, and Vᵢ = WVi · X, where WQi, WKi, and WVi are learnable weight matrices specific to the i-th attention head.\n",
    "\n",
    "2. Similarity Calculation:\n",
    "   - Compute the similarity scores for each attention head: Sᵢ = Qᵢᵀ · Kᵢ, where Sᵢ represents the similarity scores for the i-th attention head.\n",
    "\n",
    "3. Attention Weights:\n",
    "   - Apply softmax to the similarity scores for each attention head to obtain attention weights: Aᵢ = softmax(Sᵢ).\n",
    "\n",
    "4. Weighted Sum:\n",
    "   - Compute the weighted sum of the value vectors for each attention head using the attention weights: Cᵢ = Aᵢ · Vᵢ.\n",
    "\n",
    "5. Concatenation and Projection:\n",
    "   - Concatenate the outputs from all attention heads: C = [C₁, C₂, ..., Cₖ], where k is the total number of attention heads.\n",
    "   - Apply a linear transformation to the concatenated outputs: Y = WO · C, where WO is a learnable weight matrix.\n",
    "\n",
    "Here, Y represents the final output of the multi-head attention mechanism, which incorporates information from multiple attention heads.\n",
    "\n",
    "Note that in practice, there are often additional steps, such as layer normalization, residual connections, and feed-forward layers, applied to enhance the performance and stability of the multi-head attention mechanism. However, the steps outlined above capture the essence of the mathematical formulation for multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = input_size // num_heads\n",
    "        \n",
    "        self.query_transform = nn.Linear(input_size, input_size)\n",
    "        self.key_transform = nn.Linear(input_size, input_size)\n",
    "        self.value_transform = nn.Linear(input_size, input_size)\n",
    "        self.output_transform = nn.Linear(input_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into multiple heads\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        queries = self.query_transform(x).view(batch_size, seq_len, self.num_heads, self.head_size)\n",
    "        keys = self.key_transform(x).view(batch_size, seq_len, self.num_heads, self.head_size)\n",
    "        values = self.value_transform(x).view(batch_size, seq_len, self.num_heads, self.head_size)\n",
    "        \n",
    "        # Transpose dimensions for matrix multiplication\n",
    "        queries = queries.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_size)\n",
    "        keys = keys.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_size)\n",
    "        values = values.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_size)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Compute attention weights using softmax\n",
    "        weights = F.softmax(scores / (self.head_size ** 0.5), dim=-1)\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        weighted_sum = torch.matmul(weights, values)  # (batch_size, num_heads, seq_len, head_size)\n",
    "        \n",
    "        # Transpose and reshape for concatenation\n",
    "        weighted_sum = weighted_sum.transpose(1, 2).contiguous()  # (batch_size, seq_len, num_heads, head_size)\n",
    "        weighted_sum = weighted_sum.view(batch_size, seq_len, -1)  # (batch_size, seq_len, input_size)\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        output = self.output_transform(weighted_sum)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 10, 64])\n",
      "Output shape: torch.Size([32, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 64\n",
    "sequence_length = 10\n",
    "batch_size = 32\n",
    "num_heads = 8\n",
    "\n",
    "# Generate random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "# Create and apply multi-head attention\n",
    "attention = MultiHeadAttention(input_size, num_heads)\n",
    "output = attention(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
